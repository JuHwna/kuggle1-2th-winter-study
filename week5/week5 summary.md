## 6. 규제 선형 모델 - 릿지,라쏘,엘라스틱넷
### 규제 선형 모델의 개요
 - 좋은 머신러닝 회귀 모델의 특징 
   - 다항 회귀에서 Degree가 1인 경우 지나치게 예측 곡선을 단순화해 데이터에 적합하지 않는 과소적합 모델
   - Degree 15의 경우, 지나치게 모든 데이터에 적합한 회귀식을 만들기 위해서 다항식이 복잡해지고 회귀 계수가 매우 크게 설정되면서
     평가 데이터 세트에 대해서 형편없는 예측 성능을 보임
   - 회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있어야 함
 - RSS를 최소화하는, 즉 실제 값과 예측 값의 차이를 최소화하는 것만 고려함
   - 학습 데이터에 지나치게 맞추게 되고 최귀 계수가 쉽게 커짐
   - 변동성이 오히려 심해져서 테스트 데이터 세트에서는 예측 성능이 저하되기 쉬웠음
   - 이를 반영해 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록
     하는 방법이 서로 균형을 이뤄야 함
   - 회귀 계수의 크기를 제어해 과적합을 개선하기 위한 비용 함수의 목표
     - Min(RSS(W)+alpha*||W||<sup>2</sup><sub>2</sub>
     - alpha : 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터
     - 비용 함수의 목표가 최소화하는 W벡터를 찾는 것일 때 alpha가 어떤 역할을 하는지 살펴봐야 함
     - alpha=0(매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W)+0)이 됨
     - alpha가 무한대라면(또는 매우 큰 값)라면 비용 함수 식은 RSS(W)에 비해 alpha*||W||<sup>2</sup><sub>2</sub> 값이 너무 커지게 되므로
       W 값을 0(또는 매우 작게)으로 만들어야 cost가 최소화되는 비용 함수 목표를 달성할 수 있음
   - alpha 값을 크게 하면 비용 함수는 회귀 계수 W의 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W의 값이 커져도
     어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있음
   - alpha를 0에서부터 지속적으로 값을 증가시키면 회귀 계수 값의 크기를 감소시킬 수 있음
     - 규제 : 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식
     - 규제의 구분 : L2 방식, L1 방식
       - L2 규제 : alpha*||W||<sup>2</sup><sub>2</sub>와 같이 w의 제곱에 대해 패널티를 부여하는 방식
         - 릿지 회귀 : L2 규제를 적용한 회귀  
       - L1 규제 : alpha*||W||<sub>1</sub>와 같이 W의 절대값에 대해 패널티를 부여
         - 라쏘 회귀 : L1 규제를 적용한 회귀
         - L1 규제를 적용하면 영향력이 크지 않는 회귀 계수 값을 0으로 변환함

### 릿지 회귀
 - 사이킷런은 Ridge 클래스를 통해 릿지 회귀 구현
 - Ridge 클래스의 주요 생성 파라미터는 alpha이며, 이는 릿지 회귀의 alpha L2 규제 계수에 해당함
 - 릿지 회귀는 alpha 값이 커질수록 회귀 계수 값을 작게 만듦.
 - 릿지 회귀의 경우에는 회귀 계수를 0으로 만들지 않음

### 라쏘 회귀
 - 라쏘 회귀 : W의 절댓값에 패널티를 부여하는 L1 규제를 선형 회귀에 적용한 것
   - L1 규제 : alpha*||W||<sub>1</sub>
   - 라쏘 회귀 비용함수의 목표 : RSS(W)+alpha*||W||<sub>1</sub> 식을 최소화하는 W를 찾는 것
   - L2 규제가 회귀 계수의 크기를 감소시키는데 반해, L1 규제는 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거함
 - 사이킷런 Lasso 클래스를 통해 라쏘 회귀를 구현
   - Lasso 클래스의 주요 생성파라미터 : alpha이며, 이는 라쏘 회귀의 alpha L1 규제 계수에 해당
   - alpha의 크기가 증가함에 따라 일부 피처의 회귀 계수는 아예 0으로 바뀌고 있음
   - 회귀 계수가 0인 피처는 회귀 식에서 제외되면서 피처 선택의 효과를 얻을 수 있음

### 엘라스틱넷 회귀
 - 엘라스틱넷 회귀 : L2 규제와 L1 규제를 결합한 회귀
 - 엘라스틱넷 회귀 비용함수의 목표 : RSS(W)+alpha2*||W||<sup>2</sup><sub>2</sub>+alpha1*||W||<sub>1</sub>식을 최소화하는 W를 찾는 것
 - 라쏘 회귀가 서로 상관관계가 높은 피처들의 경우에 이들 중에서 중요 피처만을 셀렉션하고 다른 피처들은 모두 회귀 계수를 0으로 만드는 성향이 강함
   - 이러한 성향으로 인해 alpha값에 따라 회귀 계수의 값이 급격히 변동할 수 있는데 엘라스틱넷 회귀는 이를 완화하기 위해 L2 규제를 라쏘 회귀에 추가한 것
 - 엘라스틱넷 회귀의 단점 : L1과 L2 규제가 결합된 규제로 인해 수행 시간이 상대적으로 오래 걸림
 - 사이킷런 ElasticNet 클래스를 통해서 엘라스틱넷 회귀를 구현함
   - ElasticNet 클래스의 주요 생성 파라미터 : alpha와 l1_ratio
   - ElasticNet 클래스의 alpha는 Ridge와 Lasso 클래스의 alpha값과는 다름
   - 엘라스틱넷의 규제 : a * L1 + b+ L2로 정의될 수 있음
     - a는 L1 규제의 alpha값, b는 L2 규제의 alpha 값
     - ElasticNet 클래스의 alpha 파라미터 값 : a+b
   - ElasticNet 클래스의 l1_ration 파라미터 값 : a/(a+b)
   - l1_ratio가 0이면 a가 0이므로 L2 규제와 동일함
   - l1_ratio가 1이면 b가 0이므로 L1 규제와 동일함


### 선형 회귀 모델을 위한 데이터 변환
 - 선형 회귀 모델과 같은 선형 모델 : 일반적으로 피처와 타깃값 간에 선형의 관계가 있다고 가정하고 이러한 최적의 선형함수를 찾아내 결과값을 예측함
 - 선형 회귀 모델은 피처값과 타깃값의 분포가 정규 분포 형태를 매우 선호함
   - 타깃값의 경우, 정규분포 형태가 아니라 특정값의 분포가 치우친 왜곡된 형태의 분포도일 경우, 예측 성능에 부정적인 영향을 미칠 가능성이 높음
   - 피처값 역시 결정값보다는 덜하지만 왜곡된 분포도로 인해 예측 성능에 부정적인 영향을 미칠 가능성이 있음
 - 선형 회귀 모델을 적용하기 전에 먼저 데이터에 대한 스케일링/ 정규화 작업을 수행하는 것이 일반적
   - 스케일링/정규화 작업을 선행한다고 해서 무조건 예측 성능이 향상되는 것은 아님
   - 중요 피처들이나 타깃값의 분포도가 심하게 왜곡됐을 경우에 이러한 변환 작업을 수행
 - 피처 데이터 세트와 타깃 데이터 세트에 이러한 스케일링/정규화 작업을 수행하는 방법이 조금은 다름
 - 사이킷런을 이용해 피처 데이터 세트에 적용하는 변환 작업은 다음과 같은 방법이 있을 수 있음
   1. StandardScaler 클래스를 이용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 세트로 변환하거나 
      MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화를 수행함
   2. 스케일링/정규화를 수행한 데이터 세트에 다시 다항 특성을 적용하여 변환하는 방법입니다. 보통 1번 방법을 통해 예측 성능에 향상이 없을 경우 이와 같은 방법을 적용함
   3. 원래 값에 log 함수를 적용하면 보다 정규 분포에 가까운 형태로 값이 분표됨. 이러한 변환을 로그 변환이라고 부름
      로그 변환은 매우 유용한 변환이며 실제로 선형 회귀에서는 앞에서 소개한 1,2번 방법보다 로그 변환이 훨씬 많이 사용되는 변환 방법
      - 왜냐하면 1번 방법의 경우 예측 성능 향상을 크게 기대하기 어려운 경우가 많음
      - 2번 방법의 경우 피처의 개수가 매우 많을 경우에는 다항 변환으로 생성되는 피처의 개수가 기하급수로 늘어나서 과적합의 이슈가 발생할 수 있기 때문
      
 - 타깃값의 경우, 일반적으로 로그 변환을 적용
   - 결정 값을 정규 분포나 다른 정규값으로 변환하면 변환된 값을 다시 원본 타깃값으로 원복하기 어려울 수 있음
   - 왜곡된 분포도 형태의 타깃값을 로그 변환하여 예측 성능 향상이 된 경우가 많은 사례에서 검증되었기 때문에 타깃값의 경우 로그 변환을 적용
   
 - 선형 호기ㅟ를 적용하려는 데이터 세트에 데이터 값의 분포가 심하게 왜곡되어 있을 경우에 이처럼 로그 변환을 적용하는 것이 좋은 결과를 기대할 수 있음
 
## 7. 로지스틱 회귀
 - 로지스틱 회귀 : 선형 회귀 방식을 분류에 적용한 알고리즘
   - 분류에 사용됨
 - 로지스틱 회귀 역시 선형 회귀 계열임
 - 회귀가 선형인가 비선형인가는 독립변수가 아닌 가중치 변수가 선형인지 아닌지를 따름
 - 로지스틱 회귀가 선형 회귀와 다른 점
   - 학습을 통해 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드 함수 최적선을 찾고 이 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정한다는 것
 - 시그모이드 함수의 정의 : y=1/(1+e<sup>-x</sup>)
   - x 값이 +,-로 아무리 커지거나 작아져도 y값은 항상 0과 1 사이 값을 반환
   - x 값이 커지면 1에 근사하며 x값이 작아지면 0에 근사함
   - x가 0일 때는 0.5
 - 선형 회귀 방식을 기반으로 하되 시그모이드 함수를 이용해 분류를 수행하는 회귀
 - 로지스틱 회귀는 가볍고 빠르지만 이진 분류 예측 성능도 뛰어남
   - 이진 분류의 기본 모델로 사용하는 경우가 많음
 - 희소한 데이터 세트 분류에도 뛰어난 성능을 보여서 텍스트 분류에서도 자주 사용됨
 
## 8. 회귀 트리
 - 트리 기반의 회귀 : 회귀 트리를 이용하는 것
   - 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것
 - 회귀 트리는 분류 트리와 크게 다르지 않음
   - 차이점 : 리프 노드에서 예측 결정 값을 만드는 과정에 차이가 있음
     - 분류 트리가 특정 클래스 레이블을 결정
     - 회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측값 계산
 - 회귀 트리 동작
   - X 피처를 결정 트리 기반으로 분할하면 X 값의 균일도를 반영한 지니 계수에 따라 분할함
   - 리프 노드 생성 기준에 부합하는 트리 분할이 완료됐따면 리프 노드에 소속된 테이터 값의 평균값을 구해 최종적으로 리프 노드에 결정 값으로 할당함
 - 사이킷런의 트리 기반 회귀와 분류의 Estimator 클래스를 표로 나타낸 것
 
 |알고리즘|회귀 Estimator 클래스|분류 Estimator 클래스|
 |--------|-------------------|--------------------|
 |Decision Tree|DecisionTreeRegressor|DecisionTreeClassifier|
 |Gradient Boosting|GradientBoostingRegressor|GradientBoostingClassifier|
 |XGBoost|XGBRegressor|XGBClassifier|
 |LightGBM|LGBMRegressor|LGBMClassifier|
 
             
