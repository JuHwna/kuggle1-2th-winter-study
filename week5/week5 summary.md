## 6. 규제 선형 모델 - 릿지,라쏘,엘라스틱넷
### 규제 선형 모델의 개요
 - 좋은 머신러닝 회귀 모델의 특징 
   - 다항 회귀에서 Degree가 1인 경우 지나치게 예측 곡선을 단순화해 데이터에 적합하지 않는 과소적합 모델
   - Degree 15의 경우, 지나치게 모든 데이터에 적합한 회귀식을 만들기 위해서 다항식이 복잡해지고 회귀 계수가 매우 크게 설정되면서
     평가 데이터 세트에 대해서 형편없는 예측 성능을 보임
   - 회귀 모델은 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있어야 함
 - RSS를 최소화하는, 즉 실제 값과 예측 값의 차이를 최소화하는 것만 고려함
   - 학습 데이터에 지나치게 맞추게 되고 최귀 계수가 쉽게 커짐
   - 변동성이 오히려 심해져서 테스트 데이터 세트에서는 예측 성능이 저하되기 쉬웠음
   - 이를 반영해 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록
     하는 방법이 서로 균형을 이뤄야 함
   - 회귀 계수의 크기를 제어해 과적합을 개선하기 위한 비용 함수의 목표
     - Min(RSS(W)+alpha*||W||<sup>2</sup><sub>2</sub>
     - alpha : 학습 데이터 적합 정도와 회귀 계수 값의 크기 제어를 수행하는 튜닝 파라미터
     - 비용 함수의 목표가 최소화하는 W벡터를 찾는 것일 때 alpha가 어떤 역할을 하는지 살펴봐야 함
     - alpha=0(매우 작은 값)이라면 비용 함수 식은 기존과 동일한 Min(RSS(W)+0)이 됨
     - alpha가 무한대라면(또는 매우 큰 값)라면 비용 함수 식은 RSS(W)에 비해 alpha*||W||<sup>2</sup><sub>2</sub> 값이 너무 커지게 되므로
       W 값을 0(또는 매우 작게)으로 만들어야 cost가 최소화되는 비용 함수 목표를 달성할 수 있음
   - alpha 값을 크게 하면 비용 함수는 회귀 계수 W의 값을 작게 해 과적합을 개선할 수 있으며 alpha 값을 작게 하면 회귀 계수 W의 값이 커져도
     어느 정도 상쇄가 가능하므로 학습 데이터 적합을 더 개선할 수 있음
   - alpha를 0에서부터 지속적으로 값을 증가시키면 회귀 계수 값의 크기를 감소시킬 수 있음
     - 규제 : 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식
     - 규제의 구분 : L2 방식, L1 방식
       - L2 규제 : alpha*||W||<sup>2</sup><sub>2</sub>와 같이 w의 제곱에 대해 패널티를 부여하는 방식
         - 릿지 회귀 : L2 규제를 적용한 회귀  
       - L1 규제 : alpha*||W||<sub>1</sub>와 같이 W의 절대값에 대해 패널티를 부여
         - 라쏘 회귀 : L1 규제를 적용한 회귀
         - L1 규제를 적용하면 영향력이 크지 않는 회귀 계수 값을 0으로 변환함

### 릿지 회귀
 - 사이킷런은 Ridge 클래스를 통해 릿지 회귀 구현
 - Ridge 클래스의 주요 생성 파라미터는 alpha이며, 이는 릿지 회귀의 alpha L2 규제 계수에 해당함
 - 릿지 회귀는 alpha 값이 커질수록 회귀 계수 값을 작게 만듦.
 - 릿지 회귀의 경우에는 회귀 계수를 0으로 만들지 않음

### 라쏘 회귀
