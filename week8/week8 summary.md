## 6. 토픽 모델링 - 20 뉴스그룹
- 토픽 모델링 : 문서 집합에 숨어 있는 주제를 찾아내는 것
  - 머신러닝 기반의 토픽 모델링을 적용해 숨어 있는 중요 주제를 효과적으로 찾아낼 수 있음
  - 사람이 수행하는 토픽 모델리은 더 함축적인 의미로 문장을 요약함
  - but, 머신러닝 기반의 토픽 모델은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출함
- 머신러닝 기반의 토픽 모델링에 자주 사용되는 기법 : LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)
  - 토픽 모델링에 사용되는 LDA와 앞서 차원 축소의 LDA는 약어만 같을 뿐 서로 다른 알고리즘이므로 유의해야함
  
## 7. 문서 군집화 소개와 실습
### 문서 군집화 개념
- 문서 군집화 : 비슷한 텍스트 구성의 문서를 군집화하는 것
  - 문서 군집화는 동일한 군집에 속하는 문서를 같은 카테고리 소속으로 분류할 수 있으므로 앞에서 소개한 텍스트 분류 기반의 문서 분류와 유사함
  - 하지만 텍스트 분류 기반의 문서 분류는 사전에 결정 카테고리 값을 가진 학습 데이터 세트가 필요함
  - 문서 군집화는 학습 데이터 세트가 필요 없는 비지도학습 기반으로 동작함
  - 이전에 배웠던 군집화 기법을 활용해 텍스트 기반의 문서 군집화 적용
### Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기

## 8. 문서 유사도
### 문서 유사도 측정 방법 - 코사인 유사도
- 문서와 문서 간의 유사도 비교는 일반적으로 코사인 유사도를 사용함
  - 코사인 유사도는 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호 방향성이 얼마나 유사한지에 기반함
  - 즉, 코사인 유사도는 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것
### 두 벡터 사잇각
- 두 벡터의 사잇각에 따라서 상호 관계는 유사하거나 관련이 없거나 아예 반대 관계가 될 수 있음

![image](https://user-images.githubusercontent.com/49123169/76951488-1bebea00-694f-11ea-94af-a2ad4873724b.png)

- 두 벡터 A와 B의 코사인 값은 이 식으로 구할 수 있음
  - A * B = ||A|| ||B|| <sub>cos</sub>Θ
  - 두 벡터 A와 B의 내적 값은 두 벡터의 크기를 곱한 값의 코사인 각도 값을 곱한 것
  
- 유사도 <sub>cos</sub>Θ는 두 벡터의 내적을 총 벡터 크기의 합으로 나눈 것(내적 결과를 총 벡터 크기로 정규화(L2 Norm)한 것)

![image](https://user-images.githubusercontent.com/49123169/76951557-332ad780-694f-11ea-9be3-ef62fa473083.png)

- 코사인 유사도가 문서의 유사도 비교에 가장 많이 사용되는 이유
  - 문서를 피처 벡터화 변환하면 차원이 매우 많은 희소 행렬이 되기 쉬움
    - 이러한 희소 행렬 기반에서 문서와 문서 벡터간의 크기에 기반한 유사도 지표(ex 유클리드 거리 기반 지표)는 정확도가 떨어지기 쉬움
    - 문서가 매우 긴 경우 단어의 빈도수도 더 많을 것이기 때문에 이러한 빈도수에만 기반해서는 공정한 비교를 할 수 없음
  - 코사인 유사도는 희소 행렬이든 밀집 행렬이든 상관없이 모두 다 사용 가능함
 
## 9. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석
- 네이버 영화 평점 데이터를 기반으로 감성 분석 적용
- 한글 NLP 처리에서 주의할 점과 대표적인 파이썬 기반의 한글 형태소 패키지인 KoNLPy를 소개

