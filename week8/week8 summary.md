## 6. 토픽 모델링 - 20 뉴스그룹
- 토픽 모델링 : 문서 집합에 숨어 있는 주제를 찾아내는 것
  - 머신러닝 기반의 토픽 모델링을 적용해 숨어 있는 중요 주제를 효과적으로 찾아낼 수 있음
  - 사람이 수행하는 토픽 모델리은 더 함축적인 의미로 문장을 요약함
  - but, 머신러닝 기반의 토픽 모델은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출함
- 머신러닝 기반의 토픽 모델링에 자주 사용되는 기법 : LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)
  - 토픽 모델링에 사용되는 LDA와 앞서 차원 축소의 LDA는 약어만 같을 뿐 서로 다른 알고리즘이므로 유의해야함
  
## 7. 문서 군집화 소개와 실습
### 문서 군집화 개념
- 문서 군집화 : 비슷한 텍스트 구성의 문서를 군집화하는 것
  - 문서 군집화는 동일한 군집에 속하는 문서를 같은 카테고리 소속으로 분류할 수 있으므로 앞에서 소개한 텍스트 분류 기반의 문서 분류와 유사함
  - 하지만 텍스트 분류 기반의 문서 분류는 사전에 결정 카테고리 값을 가진 학습 데이터 세트가 필요함
  - 문서 군집화는 학습 데이터 세트가 필요 없는 비지도학습 기반으로 동작함
  - 이전에 배웠던 군집화 기법을 활용해 텍스트 기반의 문서 군집화 적용
### Opinion Review 데이터 세트를 이용한 문서 군집화 수행하기

## 8. 문서 유사도
### 문서 유사도 측정 방법 - 코사인 유사도
- 문서와 문서 간의 유사도 비교는 일반적으로 코사인 유사도를 사용함
  - 코사인 유사도는 벡터와 벡터 간의 유사도를 비교할 때 벡터의 크기보다는 벡터의 상호 방향성이 얼마나 유사한지에 기반함
  - 즉, 코사인 유사도는 두 벡터 사이의 사잇각을 구해서 얼마나 유사한지 수치로 적용한 것
### 두 벡터 사잇각
- 두 벡터의 사잇각에 따라서 상호 관계는 유사하거나 관련이 없거나 아예 반대 관계가 될 수 있음

![image](https://user-images.githubusercontent.com/49123169/76951488-1bebea00-694f-11ea-94af-a2ad4873724b.png)

- 두 벡터 A와 B의 코사인 값은 이 식으로 구할 수 있음
  - A * B = ||A|| ||B|| <sub>cos</sub>Θ
  - 두 벡터 A와 B의 내적 값은 두 벡터의 크기를 곱한 값의 코사인 각도 값을 곱한 것
  
- 유사도 <sub>cos</sub>Θ는 두 벡터의 내적을 총 벡터 크기의 합으로 나눈 것(내적 결과를 총 벡터 크기로 정규화(L2 Norm)한 것)

![image](https://user-images.githubusercontent.com/49123169/76951557-332ad780-694f-11ea-9be3-ef62fa473083.png)

- 코사인 유사도가 문서의 유사도 비교에 가장 많이 사용되는 이유
  - 문서를 피처 벡터화 변환하면 차원이 매우 많은 희소 행렬이 되기 쉬움
    - 이러한 희소 행렬 기반에서 문서와 문서 벡터간의 크기에 기반한 유사도 지표(ex 유클리드 거리 기반 지표)는 정확도가 떨어지기 쉬움
    - 문서가 매우 긴 경우 단어의 빈도수도 더 많을 것이기 때문에 이러한 빈도수에만 기반해서는 공정한 비교를 할 수 없음
  - 코사인 유사도는 희소 행렬이든 밀집 행렬이든 상관없이 모두 다 사용 가능함
 
## 9. 한글 텍스트 처리 - 네이버 영화 평점 감성 분석
- 네이버 영화 평점 데이터를 기반으로 감성 분석 적용
- 한글 NLP 처리에서 주의할 점과 대표적인 파이썬 기반의 한글 형태소 패키지인 KoNLPy를 소개

### 한글 NLP 처리의 어려움
- 일반적으로 한글 언어 처리는 영어 등의 라틴어 처리보다 어려움
  - 그 주된 원인은 '띄어쓰기'와 다양한 '조사' 때문
    - 한글은 띄어쓰기를 잘못하면 의미가 왜곡되어 전달될 수 있음
      - 띄어쓰기를 틀리는 경우가 종종 발생
    - 영어의 경우 띄어쓰기를 잘못하면 의미가 왜곡되는 게 아니라 잘못된 또는 없는 단어로 인식되는 게 대부분임
      - 영어의 띄어쓰기는 매우 명확함
    - 또 하나 중요한 이슈는 바로 '조사'임
      - 조사는 주어나 목적어를 위해 추가되며 워낙 경우의 수가 많기 때문에 어근 추출 등의 전처리 시 제거하기가 까다로움
### KoNLPy 소개
- KoNLPy는 파이썬의 대표적인 한글 형태소 패키지
- 형태소의 사전적인 의미 : '단어로서 의미를 가지는 최소 단위'로 정의
- 형태소 분석 : 말뭉치를 이러한 형태소 어근 단위로 쪼개고 각 형태소에 품사 태깅을 부착하는 작업
- KoNLPYy는 기존의 C/C++,Java로 잘 만들어진 한글 형태소 엔진을 파이썬 래퍼 기반으로 재작성한 패키지
  - 기존의 엔진은 그대로 유지한 채 파이썬 기반에서 인터페이스를 제공하기 때문에 검증된 패키지의 안전성을 유지할 수 있음
  - 꼬꼬마, 한나눔, komoran, 은전한닢 프로젝트(Mecab), twitter(okt)와 같이 5개의 형태소 분석 모듈을 KoNLPy에서 모두 사용할 수 있음
  - 안타깝게도 뛰어난 혀태소 분석으로 인정받고 있는 Mecab의 경우는 윈도우 환경에서는 구동되지 않습니다?????
  
## 10. 텍스트 분석 실습 - 캐글 Mercari Price Suggestion Challenge
- Mercari Price Suggestion Challenge는 캐글에서 진행된 Challenge로서 일본의 대형 온라인 쇼핑몰인 Mercari사의 제품에 대해 가격을 예측하는 과제
  - 제공되는 데이터 세트는 제품에 대한 여러 속성 및 제품 설명 등의 텍스트 데이터로 구성됨
  - Mercari사는 이러한 데이터를 기반으로 제품 예상 가격을 판매자들에게 제공하고자 함
  - 이와 같은 프로세스를 구현하기 위해 판매자는 제품명, 브랜드 명, 카테고리, 제품 설명 등 다양한 속성 정보를 입력하게 되고 ML 모델은 이 속성에 따라 제품의 예측 가격을 판매자에게 자동으로 제공할 수 있음
