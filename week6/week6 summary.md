# Chapter 6. 차원 축소
## 1. 차원 축소 개요
 - 대표적인 차원 축소 알고리즘 : PCA,LDA,SVD,NMF
 - 차원 축소 : 매우 많은 피처로 구성된 다차원 데이터 세트의 차원을 축소해 새로운 차원의 데이터 세트를 생성하는 것
 - 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어지게 되고 희소한 구조를 가지게 됨
 - 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 적은 차원에서 학습된 모델보다 예측 신뢰도가 떨어짐
 - 피처가 많은 경우 개별 피처 간에 상관관계가 높을 가능성이 큼
   - 선형 회귀와 같은 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 이로 인한 다중 공선성 문제로 모델의 예측 성능이 저하됨
 - 매우 많은 다차원의 피처를 차원 축소해 피처 수를 줄이면 더 직관적으로 데이터를 해석할 수 있음
 - 차원 축소를 할 경우 학습 데이터의 크기가 줄어들어서 학습에 필요한 처리 능력도 줄일 수 있음
 - 차원 축소는 피처 선택과 피처 추출로 나눌 수 있음
   - 피처 선택(특성 선택) : 말 그대로 특정 피처에 종속성이 강한 부릴요한 피처는 아예 제거하고 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것
   - 피처 추출 : 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것
     - 새롭게 추출된 중요 특성은 기존의 피처가 압축된 것이므로 기존의 피처와는 완전히 다른 값이 됨
     - 기존 피처를 단순 압축이 아닌 피처를 함축적으로 더 잘 설명할 수 있는 또 다른 공간으로 매핑해 추출하는 것
     - 함축적인 특성 추출 : 기존 피처가 전혀 인지하기 어려웠던 잠재적인 요소(Latent Factor)를 추출하는 것
 - 차원 축소
   - 단순히 데이터의 압축을 의미하는 것 X
   - 더 중요한 의미 : 차원 축소를 통해 좀 더 데이터를 잘 설명할 수 있는 잠재적인 요소를 추출하는 데 있음
   - PCA, SVD, NMF : 잠재적인 요소를 찾는 대표적인 차원 축소 알고리즘
     - 매우 많은 차원을 가지고 있는 이미지나 텍스트에서 차원 축소를 통해 잠재적인 의미를 찾아 주는 데 이 알고리즘이 잘 활용되고 있음
 - 차원 축소 알고리즘은 매우 많은 피셀로 이뤄진 이미지 데이터에서 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축을 수행할 수 있음
   - 이렇게 변환된 이미지는 원본 이미지보다 훨씬 적은 차원이기 때문에 이미지 분류 등의 분류 수행 시에 과적합 영향력이 작아져서 오히려 
     원본 데이터로 예측하는 것보다 예측 성능을 더 끌어 올릴 수 있음
     
     (그런가? 일단 이미지 수가 적어서 생기는 문제에서 쓰는 것 같은데 이미지 변조를 쓰면 해결 될 문제인 것 같긴 한데 잘 모르겠다)
     
   - 이미지 자체가 가지고 있는 차원의 수가 너무 크기 때문에 비슷한 이미지라도 적은 픽셀의 차이가 잘못된 예측으로 이어질 수 있기 때문
 - 차원 축소 알고리즘이 자주 사용되는 또 다른 영역 : 텍스트 문서의 숨겨진 의미를 추출하는 것
   - 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱 의미나 토픽을 잠재 요소로 간주하고 이를 찾아낼 수 있음
   - SVD와 NMF는 이러한 시맨틱 토픽 모델링을 위한 기반 알고리즘으로 사용됨
   
## 2. PCA(Principal Component Analysis)
### PCA 개요
 - PCA : 가장 대표적인 차원 축소 기법
   - 여러 변수 간에 존재하는 상관관계를 이용해 이를 대표하는 주성분을 추출해 차원을 축소하는 기법
   - 기존 데이터의 정보 유실이 최소화하는 것이 당연
   - 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소하는데 이것이 PCA의 주성분이 됨(즉, 분산이 데이터의 특성을 가장 잘 나타내는 것으로 간주)
   - 데이터 변동성이 가장 큰 방향으로 축을 생성하고 새롭게 생성된 축으로 데이터를 투영하는 방식
 - PCA 진행 방식
   - 제일 먼저 가장 큰 데이터 변동성을 기반으로 첫 번째 벡터 축을 생성
   - 두 번째 축은 이 벡터 축에 직각이 되는 벡터(직교 벡터)를 축으로 함
   - 세 번째 축은 다시 두 번째 축과 직각이 되는 벡터를 설정하는 방식으로 축을 생성
   - 이렇게 생성된 벡터 축에 원본 데이터를 투영하면 벡터 축의 개수만큼의 차원으로 원본 데이터가 차원 축소됨
 - PCA는 원본 데이터의 피처 개수에 비해 매우 작은 주성분으로 원본 데이터의 총 변동성을 대부분 설명할 수 있는 분석법
 - 선형대수 관점에서 해석해 보면 입력 데이터의 공분산 행렬을 고유값 분해하고 이렇게 구한 고유벡터에 입력 데이터를 선형 변환하는 것
   - 이 고유벡터가 PCA의 주성분 벡터로서 입력 데이터의 분산이 큰 방향을 나타냄
   - 고유값은 바로 이 고유벡터의 크기를 나타내며 동시에 입력 데이터의 분산을 나타냄
   - 선형 변환, 공분산 행렬과 고유벡터
     - 선형 변환 : 특정 벡터에 행렬 A를 곱해 새로운 벡터로 변환하는 것
       - 특정 벡터를 하나의 공간에서 다른 공간으로 투영하는 개념으로도 볼 수 있어 이 행렬을 공간으로 가정하는 것
     - 공분산 : 두 변수 간의 변동을 의미
       - 공분산 행렬 : 여러 변수와 관련된 공분산을 포함하는 정방형 행렬
     - 고유 벡터 : 행렬 A를 곱하더라도 방향이 변하지 않고 그 크기만 변하는 벡터
       - 고유 벡터는 여러 개가 존재, 정방 행렬은 최대 그 차원 수만큼의 고유 벡터를 가질 수 있음
       - 행렬이 작용하는 힘의 방향과 관계가 있어서 행렬을 분해하는데 사용됨
     - 공분산 행렬은 정방행렬이며 대칭행렬임
       - 정방행렬은 열과 행이 같은 행렬을 지칭
       - 대각 원소를 중심으로 원소 값이 대칭되는 행렬, A<sup>T</sup>=A인 행렬을 대칭행렬이라고 부름
       - 공분산 행렬은 개별 분산값을 대각 원소로 하는 대칭행렬
       - 대칭행렬은 고유값 분해와 관련해 매우 좋은 특성이 있음
       - 대칭행렬은 항상 고유벡터를 직교해렬로 고유값을 정방 행렬로 대각화할 수 있음
 - 선형대수식까지 써가며 강조하고 싶었던 것
   - 입력 데이터의 공분산 행렬이 고유벡터와 고유값으로 분해될 수 있음
   - 이렇게 분해된 고유벡터를 이용해 입력 데이터를 선형 변환하는 방식이 PCA라는 것
 - PCA 스텝
   1. 입력 데이터 세트의 공분산 행렬을 생성함
   2. 공분산 행렬의 고유벡터와 고유값을 계산
   3. 고유값이 가장 큰 순으로 K개(PCA 변환 차수만큼)만큼 고유벡터를 추출
   4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환함
    
  - PCA는 많은 속성으로 구성된 원본 데이터를 그 핵심을 구성하는 데이터로 압축한 것
  
## 3. LDA(Linear Discriminant Analysis)
### LDA 개요
 - LDA : 선형 판별 분석법으로 불리며 PCA와 매우 유사
   - PCA와 유사하게 입력 데이터 세트를 저차원 공간에 투영해 차원을 축소하는 기법
   - 차이점 : LDA는 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소함
     - PCA : 입력 데이터의 변동성의 가장 큰 축을 찾았음
     - LDA : 입력 데이터의 결정 값 클래스를 최대한으로 분리할 수 있는 축을 찾음
 - LDA : 특정 공간상에서 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 차원 축소
   - 클래스 간 분산은 최대한 크게 가져가고 클래스 내부의 분산은 최대한 작게 가져가는 방식
 - LDA를 구하는 스텝
   - PCA와 유사하나 가장 큰 차이점은 공분산 행렬이 아니라 위에 설명한 클래스 간 분산과 클래스 내부 분산 행렬을 생성한 뒤 이 행렬을 기반해 고유벡터를 구하고 입력 데이터를 투영한다는 점
   1. 클래스 내부와 클래스 간 분산 행렬을 구합니다. 이 두 개의 행렬은 입력 데이터의 결정 값 클래스별로 개별 피처의 평균 벡터를 기반으로 구합니다.
   2. 클래스 내부 분산 행렬을 S<sub>W</sub> 클래스 간 분산 행렬을 S<sub>B</sub>라고 하면 PCA처럼 고유벡터로 분해 가능
   3. 고유값이 가장 큰 순으로 K개(LDA변환 차수만큼) 추출합니다.
   4. 고유값이 가장 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력 데이터를 변환함
   
## 4. SVD(Singular Value Decomposition)
### SVD 개요
 - SVD : PCA와 유사한 행렬 분해 기법을 이용
   - PCA : 정방행렬(행과 열의 크기가 같은 행렬)만을 고유벡터로 분해할 수 있음
   - SVD : 정방행렬뿐만 아니라 행과 열의 크기가 다른 행렬에도 적용할 수 있음
   - SVD는 m X n 크기의 행렬 A를 다음과 같이 분해하는 것
     - A = U Σ V<sup>T</sup>
   - SVD는 특이값 분해라고 불림
     - 행렬 U와 V에 속한 벡터는 특이벡터이며, 모든 특이벡터는 서로 직교하는 성질을 가짐
     - Σ는 대각행렬 -> 0이 아닌 값이 바로 행렬 A의 특이값
     - A의 차원이 m x n일 때 U의 차원이 m x m, Σ의 차원이 m x n, V<sup>T</sup>의 차원이 n x n으로 분해
     - 일반적으로는 Σ의 비대각인 부분과 대각우너소 중에 특이값이 0인 부분도 모두 제거하고 제거된 Σ에 대응되는 U와 V원소도 함께 제거해 차원을 줄인 SVD를 적용
     - SVD 적용 시 A의 차원이 m x n일 때, U의 차원을 m x p, Σ의 차원을 p x p, V<sup>T</sup>의 차원을 p x n으로 분해
 - Truncated SVD : Σ의 대각원소 중에 상위 몇 개만 추출해서 여기에 대응하는 U와 V의 원소도 함께 제거해 더욱 차원을 줄인 형태로 분해하는 것
   - 이렇게 분해하면 인위적으로 더 작은 차원으로 분해하기 때문에 원본 행렬을 정확하게 다시 원복할 수 없음
   - 하지만 데이터 정보가 압축되어 분해됨에도 불구하고 상당한 수준으로 원본 행렬을 근사할 수 있음
   - 원래 차원의 차수에 가깝게 잘라낼수록 원본 행렬에 더 가깝게 복원할 수 있음
 
## 5. NMF(Non-Negative Matrix Factorization)
### NMF 개요
 - NMF : Truncated SVD와 같이 낮은 랭크를 통한 행렬 근사 방식의 변형
   - 원본 행렬 내의 모든 원소 값이 모두 양수(0 이상)라는 게 보장되면 좀 더 간단하게 두 개의 기반 양수 행렬로 분해될 수 있는 기법
   - 원본 행렬 V(4x6)는 행렬 W(4x2)와 행렬 H(2x6)로 근사해 분해될 수 있음
   - 행렬 분해는 일반적으로 SVD와 같은 행렬 분해 기법을 통칭함
   - 행렬 분해를 하게 되면 W 행렬과 H 행렬은 일반적으로 길고 가는 행렬 W와 작고 넓은 행렬 H로 분해됨
     - 이렇게 분해된 행렬은 잠재 요소를 특성으로 가지게 됨
     - 분해 행렬 W는 원본 행에 대해서 이 잠재 요소의 값이 얼마나 되는지에 대응
     - 분해 행렬 H는 이 잠재 요소가 원본 열(즉, 원본 속성)로 어떻게 구성됐는지를 나타내느 행렬
 - NMF도 SVD와 유사하게 이미지 압축을 통한 패턴 인식, 텍스트의 토픽 모델링 기법, 문서 유사도 및 클러스터링에 잘 사용됨
   - 영화 추천과 같은 추천 영역에 활발하게 적용
   - 사용자의 상품 평가 데이터 세트인 사용자-평가 순위 데이터 세트를 행렬 분해 기법을 통해 분해하면서 사용자가 평가하지 않는 상품에 대한
     잠재적인 요소를 추출해 이를 통해 평가 순위를 예츠하고 높은 순위로 예측된 상품을 추천해주는 방식
     
     -> 이를 잠재 요소(Latent Factoring) 기반의 추천 방식
     
# Chapter 7. 군집화
## 1. K-평균 알고리즘 이해
  - K-평균 : 군집화에서 가장 일반적으로 사용되는 알고리즘
    - K-평균은 군집 중심점이라는 특정한 임의의 지점을 선택해 해당 중심에 가장 가까운 포인트들을 선택하는 군집화 기법
    - 군집 중심점 : 선택된 포인트의 평균 지점으로 이동하고 이동된 중심점에서 다시 가까운 포인트를 선택, 다시 중심점을 평균 지점으로 이동하는 프로세스 반복
    - 모든 데이터 포인트에서 더이상 중심점의 이동이 없을 경우에 반복을 멈추고 해당 중심점에 속하는 데이터 포인트들을 군집화하는 기법
  
  |K-평균|내용|
  |------|-------|
  |장점| 1. 일반적인 군집화에서 가장 많이 활용되는 알고리즘|
  ||2. 알고리즘이 쉽고 간결함|
  |단점|1. 거리 기반 알고리즘으로 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어짐(PCA로 차원 감소를 적용해야 할 수도 있음) |
  ||2. 반복을 수행하는데 반복 횟수가 많을 경우 수행 시간이 매우 느려짐|
  ||3. 몇 개의 군집을 선택해야 할지 가이드하기가 어려움|
  
## 2. 군집 평가(Cluster Evaluation)
 - 대부분의 군집화 데이터 세트는 이렇게 비교할 만한 타깃 레이블을 가지고 있지 않음
 - 군집화는 분류와 유사해 보일 수 있으나 성격이 많이 다름
   - 데이터 내에 숨어 있는 별도의 그룹을 찾아서 의미를 부여하거나 동일한 분류 값에 속하더라도 그 안에서 더 세분화된 군집화를 추구하거나
     서로 다른 분류 값의 데이터도 더 넓은 군집화 레벨화 등의 영역을 가지고 있음
 - 군집화 평가 지표
   - 비지도학습의 특성상 어떠한 지표라도 정확하게 성능을 평가하기는 어려움
   - 군집화의 성능을 평가하는 대표적인 방법 : 실루엣 분석
   
### 실루엣 분석의 개요
 - 실루엣 분석 : 각 군집 간의 거리가 얼마나 효율적으로 분리돼 있는지를 나타냄
   - 효율적으로 잘 분리됐다는 것은 다른 군집과의 거리는 떨어져 있고 동일 군집끼리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미
   - 군집화가 잘 될수록 개별 군집은 비슷한 정도의 여유공간을 가지고 떨어져 있을 것임
 - 실루엣 계수 기반으로 평가
   - 실루엣 계수 : 개별 데이터가 가지는 군집화 지표
   - 개별 데이터가 가지는 실루엣 계수는 해당 데이터가 같은 군집 내의 데이터와 얼마나 가깝게 군집화돼 있고 다른 군집에 있는 데이터와는 얼마나 멀리 분리돼 있는지를 나타내는 지표
   - 특정 데이터 포인트의 실루엣 계수 값
     - 해당 데이터 포인트와 같은 군집 내에 있는 다른 데이터 포인트와의 거리를 평균한 값 a(i), 해당 데이터 포인트가 속하지 않은 군집 중 가장 가까운 군집과의 평균 거리 b(i)를 기반으로 계산됨
     - 두 군집 간의 거리가 얼마나 떨어져 있는가의 값 : b(i) - a(i)
     - 이 값을 정규화하기 위해 MAX(a(i)-b(i))값으로 나눔
