# Chapter 05. 회귀
## 1. 회귀 소개
 - 회귀 : 현대 통계학을 떠받치고 있는 주요 기둥 중 하나
 - 회귀 분석 : 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법
 - 통계학 용어 : 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법
 - Y=W<sub>1</sub>*X<sub>1</sub>+W<sub>2</sub>*X<sub>2</sub>+····+W<sub>n</sub>*X<sub>n</sub>
   - Y : 종속변수
   - X : 독립변수
   - W : 회귀계수(독립변수의 값에 영향을 미침)
 - 머신러닝 관점 : 독립변수는 피처에 해당, 종속변수는 결정 값
 - 머신러닝 회귀 예측의 핵심 : 주어진 피처와 결정 값 데이터 기반에서 학습을 통해 최적의 회귀 계수를 찾아내는 것
 - 회귀에서 가장 중요한 것 : 회귀 계수
 
 |독립변수 개수|회귀 계수의 결합|
 |------------|---------------|
 |1개: 단일 회귀|선형: 선형 회귀|
 |여러 개: 다중 회귀|비선형: 비선형 회귀|
 
 - 지도학습 : 분류와 회귀
   - 두 가지 기법의 가장 큰 차이 : 분류는 예측값이 카테고리와 같은 이산형 클래스 값, 회귀는 연속형 숫자 값
   
 - **선형회귀**는 실제값과 예측값의 차이(오류의 제곱 값)를 최소화하는 직선형 회귀선을 최적화하는 방식
 - 선형 회귀 보델은 규제 방법에 따라 다시 별도의 유형을 나뉠 수 있음
 - 규제 : 일반적인 선형 회귀의 과적합 문제를 해결하기 위해 회귀 계수에 페널티 값을 적용하는 것
 - 선형 회귀 모델
   - 일반 선형 회귀 : 예측값과 실제 값의 RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화하며, 규제를 적용하지 않는 모델
   - 릿지 : 선형 회귀에 L2 규제를 추가한 회귀 모델. L2 규제를 적용하는데 L2규제는 상대적으로 큰 회귀 계수 값의 예측 영향도를 감소시키기 위해
            회귀 계수 값을 더 작게 만드는 규제 모델
   - 라쏘 : 선형 회귀에 L1 규제를 적용한 방식. L2 규제가 회귀 계수 값의 크기를 줄이는데 반해, L1 규제는 예측 영향력이 작은 피처의 회귀 계수를
            0으로 만들어 회귀 예측 시 피처가 선택되지 않게 하는 것(L1 규제는 피처 선택 기능으로도 불림)
   - 엘라스틱넷 : L2,L1 규제를 함께 결합한 모델. 주로 피처가 많은 데이터 세트에서 적용되며, L1 규제로 피처의 개수를 줄임과 동시에 L2 규제로
                 계수 값의 크기를 조정
   - 로지스틱 회귀 : 분류에 사용되는 선형 모델. 일반적으로 이진 분류뿐만 아니라 희소 영역의 분류(텍스트 분류)와 같은 영역에서 뛰어난 예측 성능을 보임
   
## 2. 단순 선형 회귀를 통한 회귀 이해
 - 단순 선형 회귀는 독립변수도 하나, 종속변수도 하나인 선형 회귀
 - 독립변수가 1개인 단순 선형 회귀에서는 기울기 w1과 절편 w0을 회귀 계수로 지칭함
 - 1차 함수로 모델링했으면 w<sub>0</sub>+w<sub>1</sub>X+오류 값
 - 잔차(남은 오류) : 실제 값과 회귀 모델의 차이에 따른 오류 값
 - 최적의 회귀 모델을 만든다는 것 : 전체 데이터의 잔차(오류 값) 합이 최소가 되는 모델을 만든다는 의미
   - 오류 값 합이 최소가 될 수 있는 최적의 회귀 계수를 찾는다는 의미
 - 오류 값은 +나 1가 될 수 있음
   - 전체 데이터의 오류 합을 구하기 위해 단순히 더했다가는 뜻하지 않게 오류 합이 크게 줄어들 수 있음
   - 보통 오류 합을 계산할 때
     - MAE(Mean Absolute Error): 절댓값을 취해서 더함
     - RSS(Residual Sum of Square) : 오류 값의 제곱을 구해서 더하는 방식
     - 일반적으로 미분들의 걔산을 편리하게 하기 위해서 RSS 방식으로 오류 합을 구함
 - RSS를 최소로 하는 w<sub>0</sub>+w<sub>1</sub>, 즉 회귀 계수를 학습을 통해서 찾는 것이 머신러닝 기반 회귀의 핵심 사항
 - RSS는 회귀식의 독립변수 X, 종속변수 Y가 중심 변수가 아니라 W변수(회귀 계수)가 중심 변수임을 인지해야함
 
 ![image](https://user-images.githubusercontent.com/49123169/74443092-f7b36e00-4eb5-11ea-93eb-d528fce716c8.png)
 
 - 회귀에서 RSS는 비용이며 W 변수(회귀 계수)로 구성되는 RSS를 비용 함수라고 함
 - 머신러닝 회귀 알고리즘은 데이터를 계속 학습하면서 이 비용 함수가 반환하는 값(즉, 오류 값)을 지속해서 감소시키고
   최종적으로는 더 이상 감소하지 않는 최소의 오류 값을 구하는 것
 - 비용 함수 = 손실 함수
 
## 3. 비용 최소화하기 - 경사 하강법 소개
 - 경사 하강법 : 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 뛰어난 방식
 - 사실 경사 하강법은 '데이터를 기반으로 알고리즘이 스스로 학습한다'는 머신러닝의 개념을 가능하게 만들어준 핵심 기법의 하나
 - '점진적으로' 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식
 - 반복적으로 비용 함수의 반환 값, 즉 예측값과 실제 값<sub>1</sub>, 최소 비용으로 판단하고 그 때의 W값을 최적 파라미터로 반환
 - 핵심 : '어떻게 하면 오류가 작아지는 방향으로 W값을 보정할 수 있을까?'
 - 최초 w에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 w를 업데이트
   - 더 이상 미분된 1차 함수의 기울기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주하고 그 때의 w를 반환
 - 실제로는 위 편미분 값이 너무 클 수 있기 때문에 보정 계수를 η를 곱하는데 이를 '학습률'이라고 함
 - 경사 하강법의 일반적인 프로세스
   - Step 1 : w<sub>1</sub>,w<sub>0</sub>를 임의의 값으로 설정하고 첫 비용 함수의 값을 계산함
   - Step 2 : w<sub>1</sub>을 w<sub>1</sub>-η(2/N)<sup>N</sup>Σ<sub>i=1</sub> * (실제값<sub>i</sub>-<sub>i</sub>),
              w<sub>0</sub>을 w<sub>0</sub>-η(2/N)<sup>N</sup>Σ<sub>i=1</sub> * (실제값<sub>i</sub>-<sub>i</sub>)으로 업데이트한 후
              다시 비용 함수의 값을 계산
   - Step 3 : 비용 함수의 값이 감소했으면 다시 Step2를 반복함. 더 이상 비용 함수의 값이 감소하지 않으면 그 때의 w<sub>1</sub>,w<sub>0</sub>를 구하고 반복을 중지함
   
 - 경사 하강법 : 모든 학습 데이터에 대해 반복적으로 비용함수 최소화를 위한 값을 업데이트하기 때문에 수행 시간이 매우 오래 걸린다는 단점
 - 실전에서는 대부분 확률적 경사 하강법을 이용
 - 확률적 경사 하강법 : 전체 입력 데이터로 w가 업데이트되는 값을 계산하는 것이 아니라 일부 데이터만 이용해 w가 업데이트되는 값을 계산하므로 경사 하강법에 비해서 빠른 속도를 보장
   - 대용량의 데이터의 경우 대부분 확률적 경사 하강법이나 미니 배치 확률적 경사 하강법을 이용해 최적 비용함수를 도출함
   
## 4. 사이킷런 LinearRegression을 이용한 보스턴 주택 가격 예측
### LinearRegression 클래스 - Ordinary Least Squares
  - Ordinary Least Squares 기반의 회귀 계수 계산 : 입력 피처의 독립성에 많은 영향을 받음
    - 피처 간의 상관관계가 매우 높은 경우, 분산이 매우 커져서 오류에 매우 민감함 (다중공선성 문제)
    - 일반적으로 상관관계가 높은 피처가 많은 경우, 독립적인 중요한 피처만 남기고 제거하거나 규제를 적용
    - 매우 많은 피처가 다중 공선성 문제를 가지고 있다면 PCA를 통해 차원 추곳르 수행하는 것도 고려해 볼 수 있음
    
### 회귀 평가 지표
 - 회귀의 평가를 위한 지표 : 실제 값과 회귀 예측값의 차이 값을 기반으로 한 지표가 중심
 
 |평가지표|설명|
 |-------|----|
 |MAE|실제 값과 예측값의 차이를 절댓값으로 변환해 평균한 것|
 |MSE|실제 값과 예측값의 차이를 제곱해 평균한 것|
 |RMSE|MSE값은 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있음, MSE에 루트를 씌움|
 |R<sup>2</sup>|분산 기능으로 예측 성능을 평가, 실제 값의 분산 대비 예측값의 분산 비율을 지표로 함. 1에 가까울수록 예측 정확도가 높음|
 
 - MSE나 RMSE에 로그를 적용한 MSLE와 RMSLE도 사용함
 - 사이킷런에서는 RMSE를 제공하지 않음
 - 각 평가 방법에 대한 사이킷런의 API 및 cross_val_score나 GridSearchCV에서 평가 시 사용되는 scoring 파라미터의 적용 값
   
   |평가 방법|사이킷런 평가 지표 API|Scoring 함수 적용 값|
   |MAE|metrics.mean_absolute_error|'neg_mean_absolute_error'|
   |MSE|metrics.mean_squared_error|'neg_mean_squared_error'|
   |R<sup>2</sup>|metrics.r2_score|'r2|
   
 - cross_val_score,GridSearchCV와 같은 Scoring 함수에 회귀 평가 지표를 적용할 때 한가지 유의할 점
   - metrics.mean_absolute_error()와 같은 사이킷런 평가 지표 API는 정상적으로 양수의 값을 반환함
   - neg_mean_absolute_error가 의미하는 것은 -1* metrics.mean_absolute_error()임
   
## 5. 다항 회귀와 과(대)적합/과소적합 이해
### 다항 회귀 이해
 - 회귀가 독립변수의 단항식이 아닌 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 다항 회귀라고 함
 - 한 가지 주의할 것 : 다항 회귀를 비선형 회귀로 혼동하기 쉽지만 다항 회귀는 선형 회귀라는 점
 - 회귀에서 선형 회귀/비선형 회귀를 나누는 기준은 회귀 계수가 선형/비선형인지에 따른 것이지 독립변수의 선형/비선형 여부와는 무관함
 - 사이킷런은 다항 회귀를 위한 클래스를 명시적으로 제공하지 않음
 - 다항 회귀 역시 선형 회귀이기 때문에 비선형 함수를 선형 모델에 적용시키는 방법을 사용해 구현하면 된다

### 다항 회귀를 이용한 과소적합 및 과적합 이해
 - 다항식의 차수가 높아질수록 매우 복잡한 피처 간의 관계까지 모델링이 가능함
 - 다항 회귀의 차수를 높일수록 학습 데이터에만 너무 맞춘 학습이 이뤄져서 정작 테스트 데ㅣ터 환경에서는 오히려 예측 정확도가 떨어짐
   - 차수가 높아질수록 과적합의 문제가 크게 발생함
   
### 편항-분산 트레이드 오프
 - 편향-분산 트레이드 오프는 머신러닝이 극복해야 할 가장 중요한 이슈 중 하나
 
